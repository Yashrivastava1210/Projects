{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72dc147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3148ff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torchvision.datasets.MNIST('./data', train = True, download = True, transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.1307,),(0.3081,))]))\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = 50, shuffle = True)\n",
    "print(type(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aae178",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torchvision.datasets.MNIST('./data', train = False, download = True, transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.1307,),(0.3081,))]))\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size = 50, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fef96f4",
   "metadata": {},
   "source": [
    "## 1. Create a CNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43148be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cnn_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Cnn_model, self).__init__()\n",
    "    \n",
    "        # Convolution 1\n",
    "        self.cnn1 = nn.Conv2d(1, 16, [5, 5], [1, 1], [0, 0], [1, 1])\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # Max pool 1\n",
    "        self.maxpool1 = nn.MaxPool2d([2, 2], [1, 1], [0, 0], [1, 1])\n",
    "     \n",
    "        # Convolution 2\n",
    "        self.cnn2 = nn.Conv2d(16, 32, [5, 5], [1, 1], [0, 0], [1, 1])\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # Max pool 2\n",
    "        self.maxpool2 = nn.MaxPool2d([2, 2], [2, 2], [0, 0], [1, 1])\n",
    "        \n",
    "        # Fully connected 1\n",
    "        self.fc1 = nn.Linear(2592, 50)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        \n",
    "        # Fully connected 2\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        \n",
    "#         Forward Propagation Layer\n",
    "    def forward(self, x):\n",
    "        # Convolution 1\n",
    "        out = self.cnn1(x)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        # Max pool 1\n",
    "        out = self.maxpool1(out)\n",
    "        \n",
    "        # Convolution 2 \n",
    "        out = self.cnn2(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        # Max pool 2 \n",
    "        out = self.maxpool2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        # Linear function (readout)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu3(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9a843a",
   "metadata": {},
   "source": [
    "## 2. Train the CNN model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2be8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "model = Cnn_model()\n",
    "\n",
    "# Cross Entropy Loss Function\n",
    "error = nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD Optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e9f27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model training\n",
    "count = 0\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagation\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate softmax and ross entropy loss\n",
    "        loss = error(outputs, labels)\n",
    "        \n",
    "        # Calculating gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "        if count % 2000 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                \n",
    "                # Forward propagation\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                predicted = torch.max(outputs.data, 1)[1]\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += len(labels)\n",
    "                \n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / float(total)\n",
    "            \n",
    "            # store loss and iteration\n",
    "            loss_list.append(loss.data)\n",
    "            iteration_list.append(count)\n",
    "            accuracy_list.append(accuracy)\n",
    "            if count % 2000 == 0:\n",
    "                # Print Loss\n",
    "                print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, [loss.data], accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8b893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization loss \n",
    "plt.plot(iteration_list,loss_list)\n",
    "plt.xlabel(\"Number of iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "# plt.ylim((0.0,0.001))\n",
    "plt.title(\"CNN: Loss vs Number of iteration\")\n",
    "plt.show()\n",
    "\n",
    "# visualization accuracy \n",
    "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
    "plt.xlabel(\"Number of iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "# plt.ylim((99,99.7))\n",
    "plt.title(\"CNN: Accuracy vs Number of iteration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026398f6",
   "metadata": {},
   "source": [
    "### 3. Train the CNN model by changing the optimization algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b0f367",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in range(1,4):\n",
    "    model_for_optimizers = Cnn_model()\n",
    "    if num == 1:\n",
    "        new_optimizer = torch.optim.SGD(model_for_optimizers.parameters(), lr=0.1, momentum = 0.5)\n",
    "    elif num == 2:\n",
    "        new_optimizer = torch.optim.Adam(model_for_optimizers.parameters(), lr=0.1)\n",
    "    elif num == 3:\n",
    "        new_optimizer = torch.optim.RMSprop(model_for_optimizers.parameters(), lr=0.1)\n",
    "    \n",
    "    count = 0\n",
    "    loss_list = []\n",
    "    iteration_list = []\n",
    "    accuracy_list = []\n",
    "    for epoch in range(epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):        \n",
    "            labels = Variable(labels)                    \n",
    "            new_optimizer.zero_grad()            \n",
    "            outputs = model_for_optimizers(images)\n",
    "            loss = error(outputs, labels)\n",
    "            loss.backward()\n",
    "            new_optimizer.step()\n",
    "            count += 1\n",
    "            if count % 2000 == 0:                         \n",
    "                correct = 0\n",
    "                total = 0                \n",
    "                for images, labels in test_loader:                                    \n",
    "                    outputs = model_for_optimizers(images)                                    \n",
    "                    predicted = torch.max(outputs.data, 1)[1]                                    \n",
    "                    total += len(labels)                \n",
    "                    correct += (predicted == labels).sum()            \n",
    "                accuracy = 100 * correct / float(total)                            \n",
    "                loss_list.append(loss.data)\n",
    "                iteration_list.append(count)\n",
    "                accuracy_list.append(accuracy)\n",
    "    if num == 1:\n",
    "        print(\"CNN: GD with momentum: \")\n",
    "    elif num == 2:\n",
    "        print(\"CNN: MSProp: \")\n",
    "    elif num == 3:\n",
    "        print(\"CNN: Adam optimizers: \")            \n",
    "    \n",
    "    plt.plot(iteration_list,loss_list)\n",
    "    plt.xlabel(\"Number of iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
    "    plt.xlabel(\"Number of iteration\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6f89fa",
   "metadata": {},
   "source": [
    "The best optimizer for this problem is SGD with momentum = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7756d5f",
   "metadata": {},
   "source": [
    "### 4. Train the CNN model by changing minibatch size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2fdeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [1,100,1000,10000]\n",
    "\n",
    "for batch in batch_sizes:\n",
    "    train_var_batch = torch.utils.data.DataLoader(train, batch_size = batch, shuffle = True)\n",
    "    test_var_batch = torch.utils.data.DataLoader(test, batch_size = batch, shuffle = True)\n",
    "    \n",
    "    epochs = 10\n",
    "    model_for_batch = Cnn_model()\n",
    "    error = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model_for_batch.parameters(), lr=0.1, momentum= 0.5)\n",
    "    count = 0\n",
    "    loss_list = []\n",
    "    iteration_list = []\n",
    "    accuracy_list = []\n",
    "    time = 0\n",
    "    for epoch in range(epochs):\n",
    "        start = timeit.timeit()\n",
    "        for i, (images, labels) in enumerate(train_var_batch):        \n",
    "            labels = Variable(labels)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model_for_batch(images)\n",
    "            loss = error(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            count += 1\n",
    "            if count % 2000 == 0:\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                for images, labels in test_var_batch:                            \n",
    "                    outputs = model_for_batch(images)                            \n",
    "                    predicted = torch.max(outputs.data, 1)[1]                        \n",
    "                    total += len(labels)                \n",
    "                    correct += (predicted == labels).sum()\n",
    "                accuracy = 100 * correct / float(total)\n",
    "                loss_list.append(loss.data)\n",
    "                iteration_list.append(count)\n",
    "                accuracy_list.append(accuracy)\n",
    "        end = timeit.timeit()\n",
    "        time += (end-start)\n",
    "    print(\"Average time taken per epoch for batch size \",batch,\" is: \", time/10)\n",
    "    print(\"\\nBatch Size: \",batch)\n",
    "    plt.plot(iteration_list,loss_list)\n",
    "    plt.xlabel(\"Number of iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"CNN: Loss vs Number of iteration\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
    "    plt.xlabel(\"Number of iteration\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"CNN: Accuracy vs Number of iteration\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5a0fc3",
   "metadata": {},
   "source": [
    "The larger minibatch runs faster than smaller batch sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009ede91",
   "metadata": {},
   "source": [
    "### 5. Train the CNN model by changing learning rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874df1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.1,0.01,0.001,0.0001]\n",
    "for rate in learning_rates:\n",
    "    model_for_learning_rate = Cnn_model()\n",
    "    optimizer_var_lr = torch.optim.SGD(model_for_learning_rate.parameters(), lr=rate, momentum = 0.5)\n",
    "    count = 0\n",
    "    loss_list = []\n",
    "    iteration_list = []\n",
    "    accuracy_list = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):                            \n",
    "            optimizer_var_lr.zero_grad()            \n",
    "            outputs = model_for_learning_rate(images)\n",
    "            loss = error(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer_var_lr.step()\n",
    "            count += 1\n",
    "            if count % 2000 == 0:                         \n",
    "                correct = 0\n",
    "                total = 0                \n",
    "                for images, labels in test_loader:                                    \n",
    "                    outputs = model_for_learning_rate(images)                                    \n",
    "                    predicted = torch.max(outputs.data, 1)[1]                                    \n",
    "                    total += len(labels)                \n",
    "                    correct += (predicted == labels).sum()            \n",
    "                accuracy = 100 * correct / float(total)                            \n",
    "                loss_list.append(loss.data)\n",
    "                iteration_list.append(count)\n",
    "                accuracy_list.append(accuracy)\n",
    "    print(\"Learning Rate: \",rate)\n",
    "    plt.plot(iteration_list,loss_list)\n",
    "    plt.xlabel(\"Number of iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"CNN: Loss vs Number of iteration\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
    "    plt.xlabel(\"Number of iteration\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"CNN: Accuracy vs Number of iteration\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9209c72",
   "metadata": {},
   "source": [
    "The best learning rate in terms of convergence is 0.1 since it reaches the maximum value faster and does not deviates much after that."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
